from langchain.embeddings import HuggingFaceEmbeddings
from qdrant_client import QdrantClient
from langchain_groq import ChatGroq
from datetime import datetime
from uuid import uuid4

# Import c√°c config
import config

# --- CACHE ---
_embedding_model = None
_qdrant_client = None
# --- C√°c h√†m kh·ªüi t·∫°o v√† l·∫•y client (c√≥ cache) ---
def get_llm():
    return ChatGroq(
        model_name=config.LLM_MODEL, # L·∫•y t√™n model t·ª´ config
        api_key=config.GROQ_API_KEY, # L·∫•y API key t·ª´ config
        temperature=0.2, # Gi·∫£m nhi·ªát ƒë·ªô ƒë·ªÉ c√¢u tr·∫£ l·ªùi c·ªßa AI b·ªõt s√°ng t·∫°o, t·∫≠p trung v√†o d·ªØ li·ªáu
        max_tokens=512 # Gi·ªõi h·∫°n ƒë·ªô d√†i c·ªßa c√¢u tr·∫£ l·ªùi
    )

def get_embedding_model():
    """Kh·ªüi t·∫°o model embedding n·∫øu ch∆∞a c√≥, sau ƒë√≥ tr·∫£ v·ªÅ."""
    global _embedding_model
    if _embedding_model is None:
        _embedding_model = HuggingFaceEmbeddings(model_name=config.EMBED_MODEL)
    return _embedding_model

def get_qdrant_client():
    """Kh·ªüi t·∫°o k·∫øt n·ªëi ƒë·∫øn Qdrant n·∫øu ch∆∞a c√≥, sau ƒë√≥ tr·∫£ v·ªÅ."""
    global _qdrant_client
    if _qdrant_client is None:
        _qdrant_client = QdrantClient(host=config.QDRANT_HOST, port=config.QDRANT_PORT)
    return _qdrant_client

# --- HELPER FUNCTIONS ---
# --- C√°c h√†m tr·ª£ gi√∫p cho vi·ªác x·ª≠ l√Ω prompt v√† d·ªØ li·ªáu ---

def fuzzy_match(product_type: str, text: str) -> bool:
    """Ki·ªÉm tra xem c√°c t·ª´ trong `product_type` c√≥ xu·∫•t hi·ªán trong `text` kh√¥ng (cho ph√©p sai 1 t·ª´)."""
    if not product_type or not text:
        return False
    words = product_type.lower().split()
    matched = sum(1 for w in words if w in text.lower())
    return matched >= len(words) - 1

def generate_prompt_context(results, product_type):
    """T·∫°o ra m·ªôt chu·ªói ng·ªØ c·∫£nh t·ª´ c√°c k·∫øt qu·∫£ t√¨m ki·∫øm ƒë·ªÉ ƒë∆∞a cho LLM."""
    keywords = config.PRODUCT_KEYWORDS.get(product_type, config.DEFAULT_KEYWORDS)
    context_lines = []
    for idx, r in enumerate(results[:5]): # Gi·ªõi h·∫°n 5 s·∫£n ph·∫©m
        product_name = r.get("name_product", f"S·∫£n ph·∫©m {idx+1}")
        comment_text = r.get("comment", "") or r.get("describe", "") or r.get("detail", "") or ""
        if not comment_text.strip():
            continue
        context_lines.append(f"T√™n: {product_name}\nB√¨nh lu·∫≠n: {comment_text.strip()}\n")
    return "\n".join(context_lines)

def base_prompt(task: str, context: str, query: str):
    """T·∫°o ra c·∫•u tr√∫c prompt ho√†n ch·ªânh ƒë·ªÉ g·ª≠i ƒë·∫øn LLM."""
    if not context.strip():
        return None
    return f"""
B·∫°n l√† tr·ª£ l√Ω AI t∆∞ v·∫•n s·∫£n ph·∫©m. Tr·∫£ l·ªùi ho√†n to√†n b·∫±ng ti·∫øng Vi·ªát.
Ng∆∞·ªùi d√πng h·ªèi: \"{query}\"
D∆∞·ªõi ƒë√¢y l√† c√°c b√¨nh lu·∫≠n th·∫≠t:
{context}
üåü Nhi·ªám v·ª•:
{task}
"""

# --- QDRANT MEMORY FUNCTIONS ---
# --- C√°c h√†m qu·∫£n l√Ω "tr√≠ nh·ªõ d√†i h·∫°n" c·ªßa bot ---
def save_chat_history_to_qdrant(state, user_id: str = "default_user"):
    """L∆∞u l·∫°i cu·ªôc h·ªôi tho·∫°i hi·ªán t·∫°i v√†o Qdrant ƒë·ªÉ tham kh·∫£o trong t∆∞∆°ng lai."""
    embedding_model = get_embedding_model()
    client = get_qdrant_client()
    vector = embedding_model.embed_query(state["query"])
    products = [
        {k: p.get(k) for k in ["name_product", "price", "link_product", "link_image", "describe", "comment"]}
        for p in state.get("last_recommended_products", [])
    ]
    payload = {
        "query": state["query"],
        "response": state.get("recommendations", ""),
        "chat_history": state.get("chat_history", []),
        "user_id": user_id,
        "timestamp": datetime.utcnow().isoformat(),
        "products": products
    }
    client.upsert(
        collection_name="chat_memory",
        points=[{"id": str(uuid4()), "vector": vector, "payload": payload}]
    )

def retrieve_related_chats(query: str, top_k=3):
    """L·∫•y ra c√°c cu·ªôc h·ªôi tho·∫°i c≈© c√≥ li√™n quan ƒë·∫øn c√¢u h·ªèi hi·ªán t·∫°i."""
    embedding_model = get_embedding_model()
    client = get_qdrant_client()
    query_vector = embedding_model.embed_query(query)
    results = client.search(
        collection_name="chat_memory",
        query_vector=query_vector,
        limit=top_k,
        with_payload=True
    )
    return [r.payload for r in results]